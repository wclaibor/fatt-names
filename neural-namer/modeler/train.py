import os
import csv
import pickle
import random
import math
import numpy as np
import tensorflow as tf

from modeler.network import Network
import modeler.sampling as sampling

def sample(datafile, vocab_map, author_map):
    with open(datafile, 'r') as file_handler:
        reader = csv.reader(file_handler)
        for name, author in reader:
            seq = [1] + [vocab_map[char] for char in name]
            label = [vocab_map[char] for char in name] + [2]
            yield seq, label, [author_map[author]] * len(seq)

def train(savedir, params):
    """Train a model with the given input data file."""

    if os.path.isdir(savedir):
        # savedir exists, so load existing parameters
        with open(os.path.join(savedir, 'params.pkl'), 'rb') as fh:
            params = pickle.load(fh)
    else:
        # otherwise write the new parameters
        os.makedirs(savedir, exist_ok=True)
        with open(os.path.join(savedir, 'params.pkl'), 'wb') as fh:
            pickle.dump(params, fh)

    with open('{}.meta'.format(params.datafile), 'rb') as handler:
        # load the samples generated by the preprocessor
        vocab_map, _, author_map = pickle.load(handler)
        vocab_size = len(vocab_map)
        author_size = len(author_map)

    dataset = tf.data.Dataset.from_generator(
        lambda: sample(params.datafile, vocab_map, author_map),
        (tf.int32, tf.int32, tf.int32),
    ). \
      shuffle(buffer_size=10000). \
      repeat(params.num_epochs). \
      padded_batch(params.batch_size, ([None,], [None,], [None,]))
    
    iterator = dataset.make_one_shot_iterator()
    sequences, labels, authors = iterator.get_next()

    # initialize the network graph
    network = Network(
        sequences,
        labels,
        authors,
        vocab_size,
        author_size,
        **vars(params),
        training=True,
    )
    global_step = tf.Variable(0, name='global_step', trainable=False)
    train_step = tf.contrib.layers.optimize_loss(
        network.loss_node,
        global_step,
        params.learn_rate,
        params.optimizer,
        clip_gradients=params.grad_clip,
        learning_rate_decay_fn=lambda lr, step: tf.train.exponential_decay(
            lr, step, params.decay_steps, params.decay_rate
        )
    )

    config = tf.ConfigProto(allow_soft_placement=True)
    # begin a new tensorflow session
    sess = tf.Session(config=config)

    # create a saver to store training progress
    saver = tf.train.Saver()
    # load the saved model if it already exists
    ckpt = tf.train.get_checkpoint_state(savedir)
    if ckpt and ckpt.model_checkpoint_path:
        saver.restore(sess, ckpt.model_checkpoint_path)
    else:
        sess.run(tf.global_variables_initializer())

    # file to store model checkpoints in
    checkfile = os.path.join(savedir, 'model.ckpt')

    # track summary ops
    writer = tf.summary.FileWriter(savedir, sess.graph)
    summaries = tf.summary.merge_all()

    losses = []
    while True:
        try:
            err, summary, step, _ = sess.run(
                [network.loss_node, summaries, global_step, train_step],
            )
            losses.append(err)
            writer.add_summary(summary, step)
            if step % 100 == 0:
                saver.save(sess, os.path.join(checkfile), global_step)
                print('Step: {}, Avg loss {:.3f}'.format(step, np.mean(losses)))
                losses = []
        except (tf.errors.OutOfRangeError, KeyboardInterrupt):
            break

    saver.save(sess, os.path.join(checkfile), step)
    print('Checkpoint saved.')
